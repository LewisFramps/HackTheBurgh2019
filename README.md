Our project allows a deaf individual to use his or her webcam to record themselves producing sign language, which is translated into natural language by a CNN (with ~95% classification accuracy) hosted on Google Cloud. This is then spoken over the phone to the able bodied individual. The able bodied individual then responds by speaking, after which his or her speech is converted to sign and displayed to the deaf individual. This process then repeats itself until the conversation is complete.

Tech: ReactJS, Python, Google Cloud ML Vision, Nexmo API, IBM Watson

We encountered problems with Nexmo and IBM Watson working asynchronously.

We feel we have made a step towards making the cumbersome task of communication between deaf and non-deaf people easier. We are proud of the way in which our team operated along with the incredibly high degree of precision with which we can translate sign language. This is due to our incredibly large training set we supplied to the CNN

We learned how to use Nexmo api to convert voice to text and vice versa. We also learned a lot about sign language and through planning the project, we realised how unaccessible current solutions are in this field.
